# -*- coding: utf-8 -*-
"""AttResUnet (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Tw6p_JmULtIastg2M8P91e55hvImCng

**1. Install Required Modules**
"""

import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
import matplotlib.pyplot as plt
import tifffile as tiff
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split
import os
import numpy as np
import random
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Concatenate

# Set the seed value
seed_value = 42

# 1. Set `PYTHONHASHSEED` environment variable at a fixed value
os.environ['PYTHONHASHSEED'] = str(seed_value)

# 2. Set `python` built-in pseudo-random generator at a fixed value
random.seed(seed_value)

# 3. Set `numpy` pseudo-random generator at a fixed value
np.random.seed(seed_value)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
tf.random.set_seed(seed_value)

!pip install nibabel

!pip install -U -q segmentation-models
!pip install -q tensorflow==2.2.1
!pip install -q keras==2.5
import os
os.environ["SM_FRAMEWORK"] = "tf.keras"

from tensorflow import keras
import segmentation_models as sm
!pip install segmentation-models
import tensorflow
import keras
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda, Activation
from tensorflow.keras import models, layers, regularizers
from tensorflow.keras import backend as K
import numpy as np
import nibabel as nib
import glob
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from datetime import datetime
from segmentation_models.losses import bce_jaccard_loss
from segmentation_models.losses import bce_dice_loss
from segmentation_models.losses import binary_focal_dice_loss
from keras.losses import binary_crossentropy
#from keras.losses import dice_ce
import keras.backend as K
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
#from res_unet_model import multi_unet_model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import random
import glob
from matplotlib import pyplot as plt
import keras
import numpy as np
import os
import math

"""**2. Importing Images and Masks from Google Drive**"""

from google.colab import drive
drive.mount('/content/drive')
# Set the base path
image_files = sorted(glob.glob(('/content/drive/MyDrive/imageandsegment/training/image/*')))
label_files = sorted(glob.glob(('/content/drive/MyDrive/imageandsegment/training/mask/*')))

# Check the number of images and labels gathered
print(f"Total images: {len(image_files)}")
print(f"Total labels: {len(label_files)}")

from PIL import Image
import numpy as np
import tensorflow as tf

def preprocess_image(path):
    # Load the image using PIL library
    with Image.open(path) as img:
        # Convert the image to numpy array
        image = np.array(img)

    # If the image has more than one channel, extract just one channel
    if image.ndim > 2 and image.shape[2] > 1:
        image = image[..., 0]

    # Normalize the image to [0, 1] range
    image = image / 255.0

    # Convert image to a TensorFlow tensor
    image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)

    # Add a channel dimension if it does not exist
    if image_tensor.ndim == 2:
        image_tensor = image_tensor[..., tf.newaxis]

    # Ensure image tensor is 3D at this point
    if image_tensor.ndim != 3:
        raise ValueError('Image tensor must be 3 dimensions [height, width, channels]')

    # Resize the image to the desired size
    image_tensor = tf.image.resize(image_tensor, [256, 256])

    return image_tensor

def preprocess_mask(path):
    # Load the mask using PIL library
    with Image.open(path) as mask_img:
        mask = np.array(mask_img)

    # If the mask has more than one channel, extract just one channel
    if mask.ndim > 2 and mask.shape[2] > 1:
        mask = mask[..., 0]

    # Normalize the mask to be in [0, 1]
    mask = mask / 255.0 if mask.max() > 1 else mask

    # Convert mask to a TensorFlow tensor
    mask_tensor = tf.convert_to_tensor(mask, dtype=tf.float32)

    # Add a channel dimension if it does not exist
    if mask_tensor.ndim == 2:
        mask_tensor = mask_tensor[..., tf.newaxis]

    # Ensure mask tensor is 3D at this point
    if mask_tensor.ndim != 3:
        raise ValueError('Mask tensor must be 3 dimensions [height, width, channels]')

    # Resize the mask to the desired size
    mask_tensor = tf.image.resize(mask_tensor, [256, 256], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)

    # The resize operation could push the values away from 0 and 1, we threshold to ensure it's a proper mask
    mask_tensor = tf.where(mask_tensor > 0.5, 1, 0)

    return mask_tensor

# Subset 10% of the dataset for quick experiments
subset_size = int(0.9 * len(image_files))
image_files_subset = image_files[:subset_size]
label_files_subset = label_files[:subset_size]

# Preprocess and load images into memory (This might take a lot of RAM, be careful with large datasets)
images = np.array([preprocess_image(f) for f in image_files_subset])
masks = np.array([preprocess_mask(f) for f in label_files_subset])

# Split into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.3, random_state=42)

def conv_block(x, filter_size, size, dropout, batch_norm=False):

    conv = layers.Conv2D(size, (filter_size, filter_size), padding="same")(x)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation("relu")(conv)

    conv = layers.Conv2D(size, (filter_size, filter_size), padding="same")(conv)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation("relu")(conv)

    if dropout > 0:
        conv = layers.Dropout(dropout)(conv)

    return conv


def repeat_elem(tensor, rep):

     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),
                          arguments={'repnum': rep})(tensor)


def res_conv_block(x, filter_size, size, dropout, batch_norm=False):


    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation('relu')(conv)

    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut
    if dropout > 0:
        conv = layers.Dropout(dropout)(conv)

    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)
    if batch_norm is True:
        shortcut = layers.BatchNormalization(axis=3)(shortcut)

    res_path = layers.add([shortcut, conv])
    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)
    return res_path

def gating_signal(input, out_size, batch_norm=False):

    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)
    if batch_norm:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    return x

def attention_block(x, gating, inter_shape):
    shape_x = K.int_shape(x)
    shape_g = K.int_shape(gating)

# Getting the x signal to the same shape as the gating signal
    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16
    shape_theta_x = K.int_shape(theta_x)

# Getting the gating signal to the same number of filters as the inter_shape
    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)
    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),
                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),
                                 padding='same')(phi_g)  # 16

    concat_xg = layers.add([upsample_g, theta_x])
    act_xg = layers.Activation('relu')(concat_xg)
    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)
    sigmoid_xg = layers.Activation('sigmoid')(psi)
    shape_sigmoid = K.int_shape(sigmoid_xg)
    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32

    upsample_psi = repeat_elem(upsample_psi, shape_x[3])

    y = layers.multiply([upsample_psi, x])

    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)
    result_bn = layers.BatchNormalization()(result)
    return result_bn



def Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):
    '''
    attention  baed Rsidual UNet

    '''
    # network structure
    FILTER_NUM = 64 # number of basic filters for the first layer
    FILTER_SIZE = 3 # size of the convolutional filter
    UP_SAMP_SIZE = 2 # size of upsampling filters
    # input data
    # dimension of the image depth
    inputs = layers.Input(input_shape, dtype=tf.float32)
    axis = 3

    # Downsampling layers
    # DownRes 1, double residual convolution + pooling
    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)
    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)
    # DownRes 2
    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)
    # DownRes 3
    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)
    # DownRes 4
    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)
    # DownRes 5, convolution only
    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)

    # Upsampling layers
    # UpRes 6, attention gated concatenation + upsampling + double residual convolution
    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)
    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)
    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(conv_8)
    up_16 = layers.concatenate([up_16, att_16], axis=axis)
    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 7
    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)
    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)
    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_16)
    up_32 = layers.concatenate([up_32, att_32], axis=axis)
    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 8
    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)
    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)
    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_32)
    up_64 = layers.concatenate([up_64, att_64], axis=axis)
    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 9
    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)
    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)
    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_64)
    up_128 = layers.concatenate([up_128, att_128], axis=axis)
    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)

    # 1*1 convolutional layers

    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)
    conv_final = layers.BatchNormalization(axis=axis)(conv_final)
    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel

    # Model integration
    model = models.Model(inputs, conv_final, name="AttentionResUNet")
    return model

# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet
def dsc(y_true, y_pred):
    smooth = 1.
    y_true_f = Flatten()(y_true)
    y_pred_f = Flatten()(y_pred)
    intersection = reduce_sum(y_true_f * y_pred_f)
    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)
    return score

def dice_loss(y_true, y_pred):
    loss = 1 - dsc(y_true, y_pred)
    return loss

def bce_dice_loss(y_true, y_pred):
    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)
    return loss

# Focal Tversky loss, brought to you by:  https://github.com/nabsabraham/focal-tversky-unet
def tversky(y_true, y_pred, smooth=1e-6):
    y_true_pos = tf.keras.layers.Flatten()(y_true)
    y_pred_pos = tf.keras.layers.Flatten()(y_pred)
    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)
    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))
    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)
    alpha = 0.7
    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)

def tversky_loss(y_true, y_pred):
    return 1 - tversky(y_true,y_pred)

def focal_tversky_loss(y_true,y_pred):
    pt_1 = tversky(y_true, y_pred)
    gamma = 0.75
    return tf.keras.backend.pow((1-pt_1), gamma)

class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, list_ids, labels, image_dir, batch_size=32,
                 img_h=256, img_w=256, shuffle=True):

        self.list_ids = list_ids
        self.labels = labels
        self.image_dir = image_dir
        self.batch_size = batch_size
        self.img_h = img_h
        self.img_w = img_w
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'denotes the number of batches per epoch'
        return int(np.floor(len(self.list_ids)) / self.batch_size)

    def __getitem__(self, index):
        'generate one batch of data'
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        # get list of IDs
        list_ids_temp = [self.list_ids[k] for k in indexes]
        # generate data
        X, y = self.__data_generation(list_ids_temp)
        # return data
        return X, y

    def on_epoch_end(self):
        'update ended after each epoch'
        self.indexes = np.arange(len(self.list_ids))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_ids_temp):
        'generate data containing batch_size samples'
        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))
        y = np.empty((self.batch_size, self.img_h, self.img_w, 1))

        for idx, id in enumerate(list_ids_temp):
            file_path =  os.path.join(self.image_dir, id)
            image = cv2.imread(file_path, 0)
            image_resized = cv2.resize(image, (self.img_w, self.img_h))
            image_resized = np.array(image_resized, dtype=np.float64)
            # standardization of the image
            image_resized -= image_resized.mean()
            image_resized /= image_resized.std()

            mask = np.empty((img_h, img_w, 1))

            for idm, image_class in enumerate(['1','2','3','4']):
                rle = self.labels.get(id + '_' + image_class)
                # if there is no mask create empty mask
                if rle is None:
                    class_mask = np.zeros((256, 256))
                else:
                    class_mask = rle_to_mask(rle, width=256, height=256)

                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))
                mask[...,idm] = class_mask_resized

            X[idx,] = np.expand_dims(image_resized, axis=2)
            y[idx,] = mask

        # normalize Y
        y = (y > 0).astype(int)

        return X, y

# network configuration parameters
# original image is 1600x256, so we will resize it
img_w = 256 # resized weidth
img_h = 256 # resized height
batch_size = 12
epochs = 25
# batch size for training unet
k_size = 3 # kernel size 3x3
val_size = .20 # split of training set between train and validation set
# we will repeat the images with lower samples to make the training process more fair
repeat = False
# only valid if repeat is True
class_1_repeat = 1 # repeat class 1 examples x times
class_2_repeat = 1
class_3_repeat = 1
class_4_repeat = 1

from keras import backend as K
from keras.losses import binary_crossentropy
import tensorflow as tf

def dice_coef(y_true, y_pred, smooth=1):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def iou_coef(y_true, y_pred, smooth=1):
  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])
  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection
  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)
  return iou

def dice_loss(y_true, y_pred):
    smooth = 1.
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = y_true_f * y_pred_f
    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
    return 1. - score

def bce_dice_loss(y_true, y_pred):
    return binary_crossentropy(tf.cast(y_true, tf.float32), y_pred) + 0.5 * dice_loss(tf.cast(y_true, tf.float32), y_pred)

model = Attention_ResUNet(input_shape = (256, 256, 1))
adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)
model.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky, dice_coef,iou_coef])

from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard

# Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)

# Learning Rate Scheduler
def lr_scheduler(epoch, lr):
    decay_rate = 0.1
    decay_step = 30
    if epoch % decay_step == 0 and epoch:
        return lr * decay_rate
    return lr

lr_scheduler = LearningRateScheduler(lr_scheduler, verbose=1)

# ModelCheckpoint
checkpoint_path = "/kaggle/working/model_checkpoint.h5"
model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True)

model.summary()

import os
from keras.models import load_model

checkpoint_path = "/kaggle/working/model_checkpoint.h5"


# Function to create a new model instance
def create_model():
    input_img = Input((256, 256, 1), name='img')

    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=['accuracy', dice_coef,iou_coef])
    return model

# Check if a checkpoint exists
if os.path.exists(checkpoint_path):
    print("Loading model from checkpoint")
    model = load_model(checkpoint_path, custom_objects={'bce_dice_loss': bce_dice_loss,
                                                         'dice_coef': dice_coef,
                                                         'iou_coef': iou_coef})

    print("Model Loaded from Checkpoint")
else:
    print("Creating a new model")
    model = create_model()

from keras.callbacks import Callback    #Because Callback was not defined
from keras.layers import Add    #Because of the NameError : name 'Add' is not defined
from keras.layers import Multiply #Same for Multiply
from tensorflow.keras.layers import Conv2D, MaxPool2D
#Because of the Error : NameError: name 'MaxPool2D' is not defined
from IPython.display import clear_output    #Because of the Error : NameError: name 'clear_output' is not defined
!pip install tensorflow_addons
!pip install tensorflow_addons
!pip install keras
!pip install segmentation-models
!pip install tf_explain
clear_output()

import tensorflow_addons as tfa

from tf_explain.core.grad_cam import GradCAM
#Because of the Error NameError: name 'GradCAM' is not defined

exp = GradCAM()

class ShowProgress(Callback):
    def on_epoch_end(self, epochs, logs=None):
        id = np.random.randint(200)
        image = images[id]
        mask = masks[id]
        pred_mask = self.model.predict(image[np.newaxis,...])
        cam = exp.explain(
            validation_data=(image[np.newaxis,...], mask),
            class_index=1,
            model=self.model
        )
        plt.figure(figsize=(10,5))

        plt.subplot(1,2,1)
        plt.title("Original Mask")
        show_mask_only(mask, cmap='gist_gray')

        plt.subplot(1,2,2)
        plt.title("Predicted Mask")
        show_mask_only(pred_mask, cmap='gist_gray')

        plt.tight_layout()
        plt.show()

# Callbacks
cb = [
    # EarlyStopping(patience=3, restore_best_weight=True), # With Segmentation I trust on eyes rather than on metrics
    ModelCheckpoint("AttentionCustomUNet.h5", save_best_only=True),
    ShowProgress()
]

def show_mask_only(mask, cmap='gray', alpha=0.4):
    plt.imshow(tf.squeeze(mask), cmap=cmap, alpha=alpha)
    plt.axis('off')


def load_image(image, SIZE):
    return np.round(resize(img_to_array(load_img(image))/255.,(SIZE, SIZE)),4)


SIZE = 128

def load_images(image_paths, SIZE, mask=False, trim=None):
    if trim is not None:
        image_paths = image_paths[:trim]

    if mask:
        images = np.zeros(shape=(len(image_paths), SIZE, SIZE, 1))
    else:
        images = np.zeros(shape=(len(image_paths), SIZE, SIZE, 3))

    for i,image in enumerate(image_paths):
        img = load_image(image,SIZE)
        if mask:
            images[i] = img[:,:,:1]
        else:
            images[i] = img

    return images

results = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=60,
    validation_data=(X_val, y_val),
    callbacks=cb
)

#Because of the Error : NameError: name 'get_mask_box' is not defined

import cv2
import numpy as np

def get_mask_box(mask):
    img = np.uint8(mask[0, :, :, 0] * 255)

    _, binary = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)

    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        color = (0, 255, 0)
        cv2.rectangle(img_bgr, (x, y), (x + w, y + h), color, 2)

        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.5
        thickness = 1
        text_height = f'Height: {h}'
        text_width = f'Width: {w}'
        text_height_size = cv2.getTextSize(text_height, font, font_scale, thickness)[0]
        text_width_size = cv2.getTextSize(text_width, font, font_scale, thickness)[0]
        cv2.putText(img_bgr, text_height, (x, y - text_height_size[1]), font, font_scale, color, thickness, cv2.LINE_AA)
        cv2.putText(img_bgr, text_width, (x + w - text_width_size[0], y + h + text_width_size[1]), font, font_scale, color, thickness, cv2.LINE_AA)

    plt.imshow(img_bgr,cmap = None)
    plt.axis('off')

id = 11
image = images[id]
mask = model.predict(image[np.newaxis,...])
get_mask_box(mask)

id = 12
image = images[id]
mask = model.predict(image[np.newaxis,...])
get_mask_box(mask)

id = 13
image = images[id]
mask = model.predict(image[np.newaxis,...])
get_mask_box(mask)

id = 65
image = images[id]
mask = model.predict(image[np.newaxis,...])
get_mask_box(mask)

id = 66
image = images[id]
mask = model.predict(image[np.newaxis,...])
get_mask_box(mask)

id = 85
image = images[id]
mask = model.predict(image[np.newaxis,...])
get_mask_box(mask)

id = 193
image = images[id]
mask = model.predict(image[np.newaxis,...])
get_mask_box(mask)

import matplotlib.pyplot as plt

# Extract the history from the results
history = results.history

# Plotting Training and Validation Loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history['loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('SegNet model Loss Over Epochs')
plt.xlabel('Number of Epochs')
plt.ylabel('Loss')
plt.legend()

# Plotting Training and Validation Loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history['accuracy'], label='Training Accuracy')
plt.plot(history['val_accuracy'], label='Validation Accuracy')
plt.title('SegNet model Accuracy Over Epochs')
plt.xlabel('Number of Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plotting Training and Validation Dice Coefficient
plt.subplot(1, 2, 2)
plt.plot(history['dice_coef'], label='Train Dice Coefficient')
plt.plot(history['val_dice_coef'], label='Validation Dice Coefficient')
plt.title('SegNet model Dice Coefficient Over Epochs')
plt.xlabel('Number of Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

plt.show()

# Plotting Training and Validation IoU Coefficient
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history['iou_coef'], label='Train IoU Coefficient')
plt.plot(history['val_iou_coef'], label='Validation IoU Coefficient')
plt.title('SegNet model IoU Coefficient Over Epochs')
plt.xlabel('Number of Epochs')
plt.ylabel('IoU Coefficient')
plt.legend()

plt.show()